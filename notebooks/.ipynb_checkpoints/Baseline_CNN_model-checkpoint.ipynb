{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ff50b2-cd39-423f-8ce2-07364b73f7dd",
   "metadata": {},
   "source": [
    "## Code for sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cf306-ad17-49ae-b07a-11796983585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Class labels from train audio\n",
    "class_labels = sorted(os.listdir('/kaggle/input/birdclef-2025/train_audio/'))\n",
    "\n",
    "# List of test soundscapes (only visible during submission)\n",
    "test_soundscape_path = '/kaggle/input/birdclef-2025/test_soundscapes/'\n",
    "test_soundscapes = [os.path.join(test_soundscape_path, afile) for afile in sorted(os.listdir(test_soundscape_path)) if afile.endswith('.ogg')]\n",
    "\n",
    "# Open each soundscape and make predictions for 5-second segments\n",
    "# Use pandas df with 'row_id' plus class labels as columns\n",
    "predictions = pd.DataFrame(columns=['row_id'] + class_labels)\n",
    "for soundscape in test_soundscapes:\n",
    "\n",
    "    # Load audio\n",
    "    sig, rate = librosa.load(path=soundscape, sr=None)\n",
    "\n",
    "    # Split into 5-second chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(sig), rate*5):\n",
    "        chunk = sig[i:i+rate*5]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    # Make predictions for each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        \n",
    "        # Get row id  (soundscape id + end time of 5s chunk)      \n",
    "        row_id = os.path.basename(soundscape).split('.')[0] + f'_{i * 5 + 5}'\n",
    "        \n",
    "        # Make prediction (let's use random scores for now)\n",
    "        # scores = model.predict...\n",
    "        scores = np.random.rand(len(class_labels))\n",
    "        \n",
    "        # Append to predictions as new row\n",
    "        new_row = pd.DataFrame([[row_id] + list(scores)], columns=['row_id'] + class_labels)\n",
    "        predictions = pd.concat([predictions, new_row], axis=0, ignore_index=True)\n",
    "        \n",
    "# Save prediction as csv\n",
    "predictions.to_csv('submission.csv', index=False)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c502407-7efe-45e7-a8fe-625373a7467e",
   "metadata": {},
   "source": [
    "## My Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "404cf7aa-095d-4c7e-9e1c-8223a8d5a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fb13ef3-0c64-4da9-af5f-c1aec451ee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS support: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================\n",
    "# 1. Configuration\n",
    "# ======================\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"MPS support: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "AUDIO_DIR = '../data/train_audio/'\n",
    "SR = 32000\n",
    "DURATION = 5\n",
    "N_MELS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ddedc51-113f-4e82-b8cc-51ab3af91b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. Data Preparation\n",
    "# ======================\n",
    "\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, label_encoder, sr=SR, duration=DURATION, n_mels=N_MELS):\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_encoder = label_encoder\n",
    "        self.labels = self.label_encoder.transform(df['primary_label'])\n",
    "        self.sr = sr\n",
    "        self.audio_len = sr * duration\n",
    "        \n",
    "        # Pre-compute file paths\n",
    "        self.file_paths = [\n",
    "            os.path.join(audio_dir, row['filename'])\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Audio transforms\n",
    "        self.mel_spec = MelSpectrogram(\n",
    "            sample_rate=sr,\n",
    "            n_fft=2048,\n",
    "            win_length=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels,\n",
    "            f_min=500,\n",
    "            f_max=14000\n",
    "        )\n",
    "        self.to_db = AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        waveform, _ = torchaudio.load(self.file_paths[idx])\n",
    "        waveform = self._fix_length(waveform)\n",
    "        \n",
    "        # Feature extraction\n",
    "        spec = self.to_db(self.mel_spec(waveform))\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        \n",
    "        return spec, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "    def _fix_length(self, waveform):\n",
    "        if waveform.shape[1] > self.audio_len:\n",
    "            return waveform[:, :self.audio_len]\n",
    "        return F.pad(waveform, (0, self.audio_len - waveform.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "655b3134-c6c8-4925-8827-8440e57860c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 3. Model Architecture\n",
    "# ======================\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090318c-9e30-4694-80b0-b6e0a5cc7488",
   "metadata": {},
   "source": [
    "## Use both train_audio and train_soundscapes for fitting model and validating\n",
    "\n",
    "## Make sure filepaths contain data for soundscapes as well as train audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b367b-39b8-4d78-9b32-f40ed1aeb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. Training Pipeline\n",
    "# ======================\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(\"../data/train.csv\")  # Update path for Kaggle\n",
    "\n",
    "# Initialize encoder\n",
    "le = LabelEncoder()\n",
    "df['label_idx'] = le.fit_transform(df['primary_label'])\n",
    "\n",
    "# Split data (stratified)\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['primary_label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BirdCLEFDataset(train_df, AUDIO_DIR, le)\n",
    "val_dataset = BirdCLEFDataset(val_df, AUDIO_DIR, le)\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Initialize model\n",
    "model = BasicCNN(len(le.classes_)).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Val Acc={val_correct/len(val_dataset):.4f}\")\n",
    "\n",
    "# Save for submission\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92d1eb08-4a64-4f7b-b34f-8fdc6939732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 22851\n",
      "Validation samples: 5713\n",
      "Unique species in train: 206\n",
      "Unique species in val: 191\n"
     ]
    }
   ],
   "source": [
    "# 3. Split into train/validation sets (stratified by species)\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,                # 20% for validation\n",
    "    stratify=df['primary_label'], # Preserve class balance\n",
    "    random_state=42               # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Unique species in train: {train_df['primary_label'].nunique()}\")\n",
    "print(f\"Unique species in val: {val_df['primary_label'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91655b5f-df66-42d6-aec1-c7e5b8dbe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SAME audio_dir for both train and val\n",
    "train_dataset = BirdCLEFDataset(train_df, audio_dir=\"../data/train_audio/\", label_encoder=le)\n",
    "val_dataset = BirdCLEFDataset(val_df, audio_dir=\"../data/train_audio/\", label_encoder=le)\n",
    "\n",
    "# Save encoder for inverse mapping later\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "\n",
    "model = BasicCNN(n_classes=df['label_idx'].nunique()).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9435ef0f-0709-48d7-b114-b4ac2f967d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved validation loop\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "837ecfc2-37a0-4ac0-a7e4-386cd6db2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.7716\n",
      "Epoch 2, Loss: 4.6761\n",
      "Epoch 3, Loss: 4.4630\n",
      "Epoch 4, Loss: 4.2598\n",
      "Epoch 5, Loss: 4.0495\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(5):  # adjust as needed\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5cb2eec-f44a-4f29-aa35-22de7f5d6506",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading audio file: failed to open file ../data/train_soundscapes/pirfly1/XC333930.ogg",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mBirdCLEFDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     30\u001b[39m file_path = os.path.join(\u001b[38;5;28mself\u001b[39m.audio_dir, row[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     31\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m waveform, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m waveform = \u001b[38;5;28mself\u001b[39m._fix_length(waveform)\n\u001b[32m     36\u001b[39m mel_spec = \u001b[38;5;28mself\u001b[39m.mel_transform(waveform).squeeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# shape: [n_mels, time]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torchaudio/_backend/utils.py:205\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m backend = dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torchaudio/_backend/sox.py:44\u001b[39m, in \u001b[36mSoXBackend.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSoX backend does not support loading from file-like objects. \u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use an alternative backend that does support loading from file-like objects, e.g. FFmpeg.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     )\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     ret = \u001b[43msox_ext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_audio_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load audio from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/_ops.py:1123\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Error loading audio file: failed to open file ../data/train_soundscapes/pirfly1/XC333930.ogg"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        y_true = le.transform(y.cpu)\n",
    "        outputs = model(x)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        correct += (preds == y_true).sum()\n",
    "        total += len(y)\n",
    "\n",
    "print(f\"Validation Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb4d11-1b8d-4149-8d39-398d92601c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
