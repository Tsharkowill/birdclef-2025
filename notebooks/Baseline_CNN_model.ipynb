{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ff50b2-cd39-423f-8ce2-07364b73f7dd",
   "metadata": {},
   "source": [
    "## Code for sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cf306-ad17-49ae-b07a-11796983585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Class labels from train audio\n",
    "class_labels = sorted(os.listdir('/kaggle/input/birdclef-2025/train_audio/'))\n",
    "\n",
    "# List of test soundscapes (only visible during submission)\n",
    "test_soundscape_path = '/kaggle/input/birdclef-2025/test_soundscapes/'\n",
    "test_soundscapes = [os.path.join(test_soundscape_path, afile) for afile in sorted(os.listdir(test_soundscape_path)) if afile.endswith('.ogg')]\n",
    "\n",
    "# Open each soundscape and make predictions for 5-second segments\n",
    "# Use pandas df with 'row_id' plus class labels as columns\n",
    "predictions = pd.DataFrame(columns=['row_id'] + class_labels)\n",
    "for soundscape in test_soundscapes:\n",
    "\n",
    "    # Load audio\n",
    "    sig, rate = librosa.load(path=soundscape, sr=None)\n",
    "\n",
    "    # Split into 5-second chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(sig), rate*5):\n",
    "        chunk = sig[i:i+rate*5]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    # Make predictions for each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        \n",
    "        # Get row id  (soundscape id + end time of 5s chunk)      \n",
    "        row_id = os.path.basename(soundscape).split('.')[0] + f'_{i * 5 + 5}'\n",
    "        \n",
    "        # Make prediction (let's use random scores for now)\n",
    "        # scores = model.predict...\n",
    "        scores = np.random.rand(len(class_labels))\n",
    "        \n",
    "        # Append to predictions as new row\n",
    "        new_row = pd.DataFrame([[row_id] + list(scores)], columns=['row_id'] + class_labels)\n",
    "        predictions = pd.concat([predictions, new_row], axis=0, ignore_index=True)\n",
    "        \n",
    "# Save prediction as csv\n",
    "predictions.to_csv('submission.csv', index=False)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c502407-7efe-45e7-a8fe-625373a7467e",
   "metadata": {},
   "source": [
    "## My Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "404cf7aa-095d-4c7e-9e1c-8223a8d5a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fb13ef3-0c64-4da9-af5f-c1aec451ee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS support: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================\n",
    "# 1. Configuration\n",
    "# ======================\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"MPS support: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "AUDIO_DIR = '../data/train_audio/'\n",
    "SR = 32000\n",
    "DURATION = 5\n",
    "N_MELS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ddedc51-113f-4e82-b8cc-51ab3af91b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. Data Preparation\n",
    "# ======================\n",
    "\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, label_encoder, sr=SR, duration=DURATION, n_mels=N_MELS):\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_encoder = label_encoder\n",
    "        self.labels = self.label_encoder.transform(df['primary_label'])\n",
    "        self.sr = sr\n",
    "        self.audio_len = sr * duration\n",
    "        \n",
    "        # Pre-compute file paths\n",
    "        self.file_paths = [\n",
    "            os.path.join(audio_dir, row['filename'])\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Audio transforms\n",
    "        self.mel_spec = MelSpectrogram(\n",
    "            sample_rate=sr,\n",
    "            n_fft=2048,\n",
    "            win_length=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels,\n",
    "            f_min=500,\n",
    "            f_max=14000\n",
    "        )\n",
    "        self.to_db = AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        waveform, _ = torchaudio.load(self.file_paths[idx])\n",
    "        waveform = self._fix_length(waveform)\n",
    "        \n",
    "        # Feature extraction\n",
    "        spec = self.to_db(self.mel_spec(waveform))\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        \n",
    "        return spec, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "    def _fix_length(self, waveform):\n",
    "        if waveform.shape[1] > self.audio_len:\n",
    "            return waveform[:, :self.audio_len]\n",
    "        return F.pad(waveform, (0, self.audio_len - waveform.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "655b3134-c6c8-4925-8827-8440e57860c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 3. Model Architecture\n",
    "# ======================\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a7b367b-39b8-4d78-9b32-f40ed1aeb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. Training Class\n",
    "# ======================\n",
    "\n",
    "class BirdCLEFTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=2)\n",
    "        self.best_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in self.train_loader:\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        return running_loss / len(self.train_loader)\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                val_loss += self.criterion(outputs, labels).item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        return val_loss / len(self.val_loader), correct / total\n",
    "    \n",
    "    def run(self, max_epochs=50, patience=5):\n",
    "        no_improve = 0\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_acc = self.validate()\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                no_improve = 0\n",
    "                best_epoch = epoch\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'loss': val_loss,\n",
    "                    'accuracy': val_acc,\n",
    "                }, 'best_model.pth')\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            \n",
    "            # Save periodic checkpoints\n",
    "            if epoch % 5 == 0:\n",
    "                torch.save(self.model.state_dict(), f'checkpoint_epoch_{epoch}.pth')\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{max_epochs}: '\n",
    "                  f'Train Loss: {train_loss:.4f} | '\n",
    "                  f'Val Loss: {val_loss:.4f} | '\n",
    "                  f'Val Acc: {val_acc:.4f} | '\n",
    "                  f'LR: {self.optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "        \n",
    "        self.plot_progress(best_epoch)\n",
    "        return best_epoch\n",
    "    \n",
    "    def plot_progress(self, best_epoch):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Val Loss')\n",
    "        plt.axvline(best_epoch, color='r', linestyle='--', label='Best Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('training_curve.png')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72bb4d11-1b8d-4149-8d39-398d92601c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/h2v7b_cn2ns6mjck91vfz0_h0000gn/T/ipykernel_90932/4005924582.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/Users/tsharkowill/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss: 4.7830 | Val Loss: 4.7281 | Val Acc: 0.0282 | LR: 3.00e-04\n",
      "Epoch 2/50: Train Loss: 4.7362 | Val Loss: 4.7294 | Val Acc: 0.0347 | LR: 3.00e-04\n",
      "Epoch 3/50: Train Loss: 4.7283 | Val Loss: 4.7182 | Val Acc: 0.0371 | LR: 3.00e-04\n",
      "Epoch 4/50: Train Loss: 4.7049 | Val Loss: 4.6802 | Val Acc: 0.0410 | LR: 3.00e-04\n",
      "Epoch 5/50: Train Loss: 4.6418 | Val Loss: 4.5857 | Val Acc: 0.0488 | LR: 3.00e-04\n",
      "Epoch 6/50: Train Loss: 4.5474 | Val Loss: 4.5255 | Val Acc: 0.0637 | LR: 3.00e-04\n",
      "Epoch 7/50: Train Loss: 4.4819 | Val Loss: 4.4537 | Val Acc: 0.0725 | LR: 3.00e-04\n",
      "Epoch 8/50: Train Loss: 4.4282 | Val Loss: 4.4057 | Val Acc: 0.0812 | LR: 3.00e-04\n",
      "Epoch 9/50: Train Loss: 4.3801 | Val Loss: 4.3612 | Val Acc: 0.0866 | LR: 3.00e-04\n",
      "Epoch 10/50: Train Loss: 4.3295 | Val Loss: 4.3239 | Val Acc: 0.0917 | LR: 3.00e-04\n",
      "Epoch 11/50: Train Loss: 4.2753 | Val Loss: 4.2739 | Val Acc: 0.1019 | LR: 3.00e-04\n",
      "Epoch 12/50: Train Loss: 4.2250 | Val Loss: 4.2155 | Val Acc: 0.1189 | LR: 3.00e-04\n",
      "Epoch 13/50: Train Loss: 4.1690 | Val Loss: 4.1822 | Val Acc: 0.1229 | LR: 3.00e-04\n",
      "Epoch 14/50: Train Loss: 4.1196 | Val Loss: 4.1349 | Val Acc: 0.1294 | LR: 3.00e-04\n",
      "Epoch 15/50: Train Loss: 4.0718 | Val Loss: 4.1078 | Val Acc: 0.1278 | LR: 3.00e-04\n",
      "Epoch 16/50: Train Loss: 4.0251 | Val Loss: 4.0642 | Val Acc: 0.1385 | LR: 3.00e-04\n",
      "Epoch 17/50: Train Loss: 3.9822 | Val Loss: 4.0263 | Val Acc: 0.1502 | LR: 3.00e-04\n",
      "Epoch 18/50: Train Loss: 3.9392 | Val Loss: 4.0055 | Val Acc: 0.1523 | LR: 3.00e-04\n",
      "Epoch 19/50: Train Loss: 3.9037 | Val Loss: 3.9814 | Val Acc: 0.1591 | LR: 3.00e-04\n",
      "Epoch 20/50: Train Loss: 3.8688 | Val Loss: 3.9399 | Val Acc: 0.1563 | LR: 3.00e-04\n",
      "Epoch 21/50: Train Loss: 3.8285 | Val Loss: 3.9175 | Val Acc: 0.1626 | LR: 3.00e-04\n",
      "Epoch 22/50: Train Loss: 3.7943 | Val Loss: 3.8626 | Val Acc: 0.1787 | LR: 3.00e-04\n",
      "Epoch 23/50: Train Loss: 3.7582 | Val Loss: 3.8703 | Val Acc: 0.1794 | LR: 3.00e-04\n",
      "Epoch 24/50: Train Loss: 3.7245 | Val Loss: 3.8317 | Val Acc: 0.1829 | LR: 3.00e-04\n",
      "Epoch 25/50: Train Loss: 3.6932 | Val Loss: 3.8060 | Val Acc: 0.1887 | LR: 3.00e-04\n",
      "Epoch 26/50: Train Loss: 3.6609 | Val Loss: 3.7637 | Val Acc: 0.1929 | LR: 3.00e-04\n",
      "Epoch 27/50: Train Loss: 3.6345 | Val Loss: 3.7846 | Val Acc: 0.1857 | LR: 3.00e-04\n",
      "Epoch 28/50: Train Loss: 3.6031 | Val Loss: 3.7172 | Val Acc: 0.2057 | LR: 3.00e-04\n",
      "Epoch 29/50: Train Loss: 3.5726 | Val Loss: 3.6955 | Val Acc: 0.2156 | LR: 3.00e-04\n",
      "Epoch 30/50: Train Loss: 3.5447 | Val Loss: 3.7032 | Val Acc: 0.2085 | LR: 3.00e-04\n",
      "Epoch 31/50: Train Loss: 3.5171 | Val Loss: 3.6559 | Val Acc: 0.2172 | LR: 3.00e-04\n",
      "Epoch 32/50: Train Loss: 3.4913 | Val Loss: 3.6377 | Val Acc: 0.2235 | LR: 3.00e-04\n",
      "Epoch 33/50: Train Loss: 3.4645 | Val Loss: 3.6053 | Val Acc: 0.2354 | LR: 3.00e-04\n",
      "Epoch 34/50: Train Loss: 3.4398 | Val Loss: 3.5946 | Val Acc: 0.2328 | LR: 3.00e-04\n",
      "Epoch 35/50: Train Loss: 3.4169 | Val Loss: 3.5923 | Val Acc: 0.2349 | LR: 3.00e-04\n",
      "Epoch 36/50: Train Loss: 3.3895 | Val Loss: 3.5644 | Val Acc: 0.2377 | LR: 3.00e-04\n",
      "Epoch 37/50: Train Loss: 3.3677 | Val Loss: 3.5366 | Val Acc: 0.2456 | LR: 3.00e-04\n",
      "Epoch 38/50: Train Loss: 3.3387 | Val Loss: 3.5239 | Val Acc: 0.2510 | LR: 3.00e-04\n",
      "Epoch 39/50: Train Loss: 3.3175 | Val Loss: 3.4949 | Val Acc: 0.2533 | LR: 3.00e-04\n",
      "Epoch 40/50: Train Loss: 3.2942 | Val Loss: 3.4809 | Val Acc: 0.2619 | LR: 3.00e-04\n",
      "Epoch 41/50: Train Loss: 3.2724 | Val Loss: 3.4708 | Val Acc: 0.2543 | LR: 3.00e-04\n",
      "Epoch 42/50: Train Loss: 3.2520 | Val Loss: 3.4515 | Val Acc: 0.2662 | LR: 3.00e-04\n",
      "Epoch 43/50: Train Loss: 3.2305 | Val Loss: 3.4294 | Val Acc: 0.2661 | LR: 3.00e-04\n",
      "Epoch 44/50: Train Loss: 3.2091 | Val Loss: 3.4077 | Val Acc: 0.2731 | LR: 3.00e-04\n",
      "Epoch 45/50: Train Loss: 3.1887 | Val Loss: 3.4041 | Val Acc: 0.2767 | LR: 3.00e-04\n",
      "Epoch 46/50: Train Loss: 3.1703 | Val Loss: 3.3906 | Val Acc: 0.2809 | LR: 3.00e-04\n",
      "Epoch 47/50: Train Loss: 3.1489 | Val Loss: 3.3908 | Val Acc: 0.2755 | LR: 3.00e-04\n",
      "Epoch 48/50: Train Loss: 3.1301 | Val Loss: 3.3438 | Val Acc: 0.2897 | LR: 3.00e-04\n",
      "Epoch 49/50: Train Loss: 3.1135 | Val Loss: 3.3281 | Val Acc: 0.2920 | LR: 3.00e-04\n",
      "Epoch 50/50: Train Loss: 3.0985 | Val Loss: 3.3098 | Val Acc: 0.2969 | LR: 3.00e-04\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::quantize_per_tensor' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 2236df1770800ffea5697b11b0bb0d910b2e59e1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m     pickle.dump(le, f)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Quantize for inference (optional)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m quantized_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqint8\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m torch.save(quantized_model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mquantized_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:562\u001b[39m, in \u001b[36mquantize_dynamic\u001b[39m\u001b[34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[39m\n\u001b[32m    560\u001b[39m model.eval()\n\u001b[32m    561\u001b[39m propagate_qconfig_(model, qconfig_spec)\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:657\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m    656\u001b[39m     module = copy.deepcopy(module)\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[32m    666\u001b[39m     _remove_qconfig(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:722\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    710\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    712\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    713\u001b[39m     ):\n\u001b[32m    714\u001b[39m         _convert(\n\u001b[32m    715\u001b[39m             mod,\n\u001b[32m    716\u001b[39m             mapping,\n\u001b[32m   (...)\u001b[39m\u001b[32m    720\u001b[39m             use_precomputed_fake_quant=use_precomputed_fake_quant,\n\u001b[32m    721\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     reassign[name] = \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n\u001b[32m    727\u001b[39m     module._modules[key] = value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:764\u001b[39m, in \u001b[36mswap_module\u001b[39m\u001b[34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    762\u001b[39m sig = inspect.signature(qmod.from_float)\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_precomputed_fake_quant\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig.parameters:\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     new_mod = \u001b[43mqmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    768\u001b[39m     new_mod = qmod.from_float(mod)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py:138\u001b[39m, in \u001b[36mLinear.from_float\u001b[39m\u001b[34m(cls, mod, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    136\u001b[39m weight_observer(mod.weight)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype == torch.qint8:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     qweight = \u001b[43m_quantize_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_observer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype == torch.float16:\n\u001b[32m    140\u001b[39m     qweight = mod.weight.float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/birdclef-2025/birdclef-env/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/utils.py:72\u001b[39m, in \u001b[36m_quantize_weight\u001b[39m\u001b[34m(float_wt, observer)\u001b[39m\n\u001b[32m     70\u001b[39m wt_scale, wt_zp = observer.calculate_qparams()\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observer.qscheme \u001b[38;5;129;01min\u001b[39;00m [torch.per_tensor_symmetric, torch.per_tensor_affine]:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     qweight = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize_per_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfloat_wt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwt_scale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwt_zp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqint8\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     qweight = _clamp_weights(qweight, observer, wt_scale, wt_zp)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m observer.qscheme \u001b[38;5;129;01min\u001b[39;00m [torch.per_channel_symmetric, torch.per_channel_affine]:\n",
      "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::quantize_per_tensor' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 2236df1770800ffea5697b11b0bb0d910b2e59e1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 5. Training Pipeline\n",
    "# ======================\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(\"../data/train.csv\")  # Update path for Kaggle\n",
    "\n",
    "# Initialize encoder\n",
    "le = LabelEncoder()\n",
    "df['label_idx'] = le.fit_transform(df['primary_label'])\n",
    "\n",
    "# Split data (stratified)\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['primary_label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BirdCLEFDataset(train_df, AUDIO_DIR, le)\n",
    "val_dataset = BirdCLEFDataset(val_df, AUDIO_DIR, le)\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Initialize model\n",
    "model = BasicCNN(len(le.classes_)).to(DEVICE)\n",
    "\n",
    "# Train\n",
    "trainer = BirdCLEFTrainer(model, train_loader, val_loader, DEVICE)\n",
    "best_epoch = trainer.run(max_epochs=50, patience=5)\n",
    "\n",
    "# Save final artifacts\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "# Quantize for inference (optional)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ae362-24a0-4a36-bab6-e2858deb6e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
